groups:
  - name: kafka_alerts
    interval: 30s
    rules:
      - alert: KafkaConsumerLagHigh
        expr: kafka_consumergroup_lag > 1000
        for: 2m
        labels:
          severity: warning
          component: kafka
        annotations:
          summary: "High Kafka consumer lag detected"
          description: "Consumer group {{ $labels.consumergroup }} has lag of {{ $value }} messages on topic {{ $labels.topic }}"

      - alert: KafkaConsumerLagCritical
        expr: kafka_consumergroup_lag > 5000
        for: 5m
        labels:
          severity: critical
          component: kafka
        annotations:
          summary: "CRITICAL: Kafka consumer lag is very high"
          description: "Consumer group {{ $labels.consumergroup }} has critical lag of {{ $value }} messages on topic {{ $labels.topic }}. Events are piling up!"

      - alert: KafkaNoMessagesProduced
        expr: rate(kafka_topic_partition_current_offset[5m]) == 0
        for: 10m
        labels:
          severity: warning
          component: kafka
        annotations:
          summary: "No messages produced to Kafka topic"
          description: "Topic {{ $labels.topic }} partition {{ $labels.partition }} has not received any messages in the last 10 minutes"

  - name: flink_alerts
    interval: 30s
    rules:
      - alert: FlinkHighBackpressure
        expr: flink_taskmanager_job_task_backPressuredTimeMsPerSecond > 500
        for: 3m
        labels:
          severity: warning
          component: flink
        annotations:
          summary: "Flink task experiencing backpressure"
          description: "Task {{ $labels.task_name }} in job {{ $labels.job_name }} has backpressure of {{ $value }}ms/s. Processing is slower than ingestion!"

      - alert: FlinkCheckpointDurationHigh
        expr: flink_jobmanager_job_lastCheckpointDuration > 120000
        for: 5m
        labels:
          severity: warning
          component: flink
        annotations:
          summary: "Flink checkpoint taking too long"
          description: "Job {{ $labels.job_name }} checkpoint duration is {{ $value }}ms (> 2 minutes). This may indicate state size issues."

      - alert: FlinkCheckpointFailed
        expr: increase(flink_jobmanager_job_numberOfFailedCheckpoints[10m]) > 2
        for: 1m
        labels:
          severity: critical
          component: flink
        annotations:
          summary: "CRITICAL: Flink checkpoints failing"
          description: "Job {{ $labels.job_name }} has {{ $value }} failed checkpoints in the last 10 minutes. Fault tolerance is compromised!"

      - alert: FlinkNoRecordsProcessed
        expr: rate(flink_taskmanager_job_task_operator_numRecordsIn[5m]) == 0
        for: 10m
        labels:
          severity: warning
          component: flink
        annotations:
          summary: "Flink operator not processing records"
          description: "Operator {{ $labels.operator_name }} in job {{ $labels.job_name }} has not processed any records in 10 minutes"

      - alert: FlinkFrequentRestarts
        expr: increase(flink_jobmanager_job_numRestarts[30m]) > 3
        for: 1m
        labels:
          severity: critical
          component: flink
        annotations:
          summary: "CRITICAL: Flink job restarting frequently"
          description: "Job {{ $labels.job_name }} has restarted {{ $value }} times in the last 30 minutes. Investigate job stability!"

  - name: backend_alerts
    interval: 30s
    rules:
      - alert: BackendHighErrorRate
        expr: rate(http_server_requests_seconds_count{status=~"5.."}[5m]) > 0.1
        for: 3m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "High HTTP error rate detected"
          description: "Backend is returning {{ $value }} 5xx errors/sec on {{ $labels.uri }}"

      - alert: BackendHighMemoryUsage
        expr: (jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"}) > 0.85
        for: 5m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "Backend JVM heap usage is high"
          description: "Heap usage is at {{ $value | humanizePercentage }}. Possible memory leak or need to increase heap size."

      - alert: BackendDown
        expr: up{job="backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: backend
        annotations:
          summary: "CRITICAL: Backend is down"
          description: "Backend API is not responding. All ticket operations are unavailable!"

  - name: system_alerts
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "CRITICAL: Service is down"
          description: "Service {{ $labels.job }} ({{ $labels.instance }}) is not responding to Prometheus scrapes"

      - alert: SlowScrapeTarget
        expr: scrape_duration_seconds > 5
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Prometheus scrape target responding slowly"
          description: "Scraping {{ $labels.job }} takes {{ $value }}s (> 5s). Target may be overloaded."
